# -*- coding: utf-8 -*-
"""NLP_DeepDive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l0Cxr_FbQ83-fU1Az-grLNuA3AYCyzw3
"""

from fastai.text.all import *

path = '/Users/phucbb/Personal/lm-hackers/vn_numbers.txt'

contents = open(path).readlines()

text = ' . '.join([c.strip() for c in contents])
print(text[:10])
vocab = sorted(list(set(text.split())))

word2idx = {w:i for i, w in enumerate(vocab)}
idx2word = {i:w for i, w in enumerate(vocab)}

encode = lambda x: [word2idx[w] for w in x]
decode = lambda x: ' '.join([idx2word[i] for i in x.tolist()])

block_size = 3
bs = 64

def group_chunks(ds, bs):
    m = len(ds) // bs
    new_ds = L()
    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))
    return new_ds

block_size = 12
tok_text = [word2idx[w] for w in text.split()]
seqs = L((tensor(tok_text[i:i+block_size]), tensor(tok_text[i+1:i+block_size+1])) for i in range(0, len(tok_text) - block_size, block_size))

cut = int(len(seqs) * 0.8)
bs = 64
dls = DataLoaders.from_dsets(
    group_chunks(seqs[:cut], bs),
    group_chunks(seqs[cut:], bs),
    bs=bs, drop_last=True, shuffle=False)

class LMModel4(Module):
  def __init__(self, vocab_sz, n_hidden):
    self.i_h = nn.Embedding(vocab_sz, n_hidden)
    self.h_h = nn.Linear(n_hidden, n_hidden)
    self.h_o = nn.Linear(n_hidden, vocab_sz)
    self.h = 0

  def forward(self, x):
    outs = []
    for i in range(block_size):
      self.h = self.h + self.i_h(x[:, i]) # self.h += self.i_h(x[:, i])
      self.h = F.relu(self.h_h(self.h))
      outs.append(self.h_o(self.h))
    self.h = self.h.detach()
    out = torch.stack(outs, dim=1)
    return out

  def reset(self): self.h=0

def loss_fnc(pred, target):
  return F.cross_entropy(pred.view(-1, len(vocab)), target.view(-1))

learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_fnc,
                metrics=accuracy, cbs=ModelResetter)
learn.fit_one_cycle(20, 3e-3)

