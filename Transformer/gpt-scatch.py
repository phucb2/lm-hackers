# -*- coding: utf-8 -*-
"""NLP_DeepDive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l0Cxr_FbQ83-fU1Az-grLNuA3AYCyzw3
"""

from fastai.text.all import *

# path = untar_data(URLs.HUMAN_NUMBERS)

# contents = []
# with open(path/'train.txt') as file:
#   contents += file.readlines()
# with open(path/'valid.txt') as file:
#   contents += file.readlines()

path = Path('vn_numbers.txt')

contents = []
# read first 1000 lines
with open(path) as file:
  contents += file.readlines()[:10000]

text = ' . '.join([c.strip() for c in contents])
vocab = sorted(list(set(text.split())))

word2idx = {w:i for i, w in enumerate(vocab)}
idx2word = {i:w for i, w in enumerate(vocab)}

encode = lambda x: [word2idx[w] for w in x]
decode = lambda x: ' '.join([idx2word[i] for i in x.tolist()])

def group_chunks(ds, bs):
    m = len(ds) // bs
    new_ds = L()
    for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs))
    return new_ds
#===============================================================================
# Parameters
#===============================================================================

# model parameters
vocab_size = len(vocab)
n_embs = 64
num_heads = 4
num_layers = 4
dropout_rate = 0.2

num_epoches = 10
learning_rate = 1e-2

bs = 128
block_size = 32 # number of characters to predict

dry_run = False

if dry_run:
  num_epoches = 1
  eval_iter = 1

#===============================================================================
# Data
#===============================================================================
tok_text = [word2idx[w] for w in text.split()]
seqs = L((tensor(tok_text[i:i+block_size]), tensor(tok_text[i+1:i+block_size+1])) for i in range(0, len(tok_text) - block_size, block_size))

cut = int(len(seqs) * 0.8)
dls = DataLoaders.from_dsets(
    group_chunks(seqs[:cut], bs),
    group_chunks(seqs[cut:], bs),
    bs=bs, drop_last=True, shuffle=True)

#===============================================================================
# Model definition
#===============================================================================

class Head(nn.Module):
  def __init__(self, head_dim):
    super().__init__()
    self.query = nn.Linear(n_embs, head_dim)
    self.key = nn.Linear(n_embs, head_dim)
    self.value = nn.Linear(n_embs, head_dim)
    self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))
    self.dropout = nn.Dropout(dropout_rate) 
  def forward(self, x):
    B, T, C = x.shape
    
    q = self.query(x)
    k = self.key(x)
    
    head_dim = k.shape[-1]
    wei = q @ k.transpose(-1, -2) # / (head_dim ** 0.5)
    wei = wei.masked_fill(self.tril[:T,:T] == 0, float('-inf'))
    wei = F.softmax(wei, dim=-1)
    wei = self.dropout(wei)
  
    v = self.value(x) # B, T, C
    out = wei @ v # (B, T, T) @ (B, T, head_size) = (B, T, head_size)
    return out

class MultiHead(nn.Module):
  def __init__(self, head_size, num_heads) -> None:
    super().__init__()
    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
    self.proj = nn.Linear(head_size * num_heads, n_embs)
    self.dropout = nn.Dropout(dropout_rate)
    
  def forward(self, x):
    out = torch.cat([h(x) for h in self.heads], dim=-1)
    out = self.dropout(out)
    return self.proj(out)
  
class Feedforward(nn.Module):
  def __init__(self):
    super().__init__()
    self.net = nn.Sequential(
      nn.Linear(n_embs, n_embs * 4),
      nn.ReLU(),
      nn.Linear(n_embs * 4, n_embs),
      nn.Dropout(dropout_rate)
    )
    
  def forward(self, x):
    return self.net(x)
  
class Block(nn.Module):
  def __init__(self):
    super().__init__()
    self.sa = MultiHead(n_embs // num_heads, num_heads)
    self.ffwd = Feedforward()
    self.ln1 = nn.LayerNorm(n_embs)
    self.ln2 = nn.LayerNorm(n_embs)
    
  def forward(self, x):
    x = x + self.sa(self.ln1(x))
    x = x + self.ffwd(self.ln2(x))
    return x
    

class GptModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.token_embd = nn.Embedding(vocab_size, n_embs)
    self.position_embd = nn.Embedding(block_size, n_embs)
    self.blocks = nn.Sequential(*[Block() for _ in range(num_layers)])
    self.lm_head = nn.Linear(n_embs, vocab_size)
    
    
  def forward(self, idx, targets=None):
    B, T = idx.shape
    token_em = self.token_embd(idx) # (B, T, C) and C = vocab_size
    position_em = self.position_embd(torch.arange(T, device=idx.device)) # (T, C)
    x = token_em + position_em # (B, T, C)
    x = self.blocks(x)
    logits = self.lm_head(x) # (B, T, C)
    return logits
  
  def generate(self,idx,max_generate):
    for _ in range(max_generate):
      idx_cond = idx[:,-block_size:] # B, T
      print(idx_cond.shape)
      print(idx)
      logits = self(idx_cond) # B, T, C
      logits = logits[:,-1,:]
      probs = F.softmax(logits, dim=-1) # B, C
      idx_next = torch.multinomial(probs, num_samples=1)
      idx = torch.cat((idx, idx_next), dim=1) # B, T+1
    return idx
  
#===============================================================================
# Training
#===============================================================================

def loss_fnc(logits, targets):
  B, T, C = logits.shape
  logits = logits.view(B*T, C) # (B*T, C)
  targets = targets.view(B*T)
  loss = F.cross_entropy(logits, targets)
  return loss

learn = Learner(dls, GptModel(), 
                # AdamW,
                opt_func=partial(Adam, decouple_wd=True),
                loss_func=loss_fnc,
                metrics=accuracy)
# learn.fit_one_cycle(num_epoches, learning_rate)

#===============================================================================
# Evaluation
#===============================================================================
# Load model from file
model = GptModel()
model.load_state_dict(torch.load('model.pth'))
# model = learn.model.eval()

def generate_text(m, max_size):
  init = torch.zeros((1, 1), dtype=torch.long)
  o = m.generate(init, max_size)
  return decode(o[0])

# print(generate_text(model, 100))

# Save model to file
# torch.save(model.state_dict(), 'model.pth')

# Predict with prompt
def predict_with_prompt(m, prompt, max_size):
  init = torch.tensor(encode(prompt.split())).unsqueeze(0)
  o = m.generate(init, max_size)
  return decode(o[0])

# promt = 'mười hai . mười ba . mười bốn .' # 12 13 14
# 1 2 3
promt = "một . hai . ba ."
o = predict_with_prompt(model, promt, 100)
print("========================================")
print(o)